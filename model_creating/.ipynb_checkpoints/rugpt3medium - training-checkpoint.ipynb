{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a44d32-6cc5-4c96-b2e7-b2d450e77a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Alex_Karachun\\DEV\\neuroPushkin-nlp_project\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Alex_Karachun\\DEV\\neuroPushkin-nlp_project\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '../../pretrained_models/hugging_face'\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9642e48-dc85-453e-9c82-9823cd0f26a1",
   "metadata": {},
   "source": [
    "доделать\n",
    "\n",
    " - переделать датасет так, чтобы после точек всегда стоял пробел\n",
    " - убрать последовательности пробелов\n",
    "учить в 1 эпоху, чтобы модель была стилизована, но могла писать не только рассказы - (сохранить все чекпоинты)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20293769-5dd2-46dd-b996-3c16d10cf44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdb486-1883-4a84-99d5-0001f4348455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "# model_path = f'sberbank-ai/rugpt3medium_based_on_gpt2_train/checkpoint-12000'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfde13-3c91-47ca-b2c4-458a9dc5af6e",
   "metadata": {},
   "source": [
    "загружаю датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09d3bf-e315-4769-bdc6-c8af463ce187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_paths, tokenizer, block_size=512, multip=20):\n",
    "    tokenized_texts = []\n",
    "    for path in file_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            texts = f.read()\n",
    "            texts = texts.split('#' * 10)\n",
    "\n",
    "            tokenized_texts.extend([tokenizer.encode(t.strip()) for t in texts if len(t) > 50])\n",
    "\n",
    "\n",
    "\n",
    "    # изображу длины тектов в токенах для наглядности параметров block_size, multip\n",
    "    lens = [len(t) for t in tokenized_texts]\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.scatter(np.arange(len(lens)), lens, c='purple', s=1)\n",
    "    plt.title('длины текстов в токенах')\n",
    "    plt.xlabel('тексты')\n",
    "    plt.ylabel('длина в токенах')\n",
    "    plt.show()\n",
    "    \n",
    "    # режу тексты на нахлестывающиеся отрезки\n",
    "    overlapped_texts = []\n",
    "    for text in tokenized_texts:\n",
    "        if len(text) > block_size:\n",
    "            step = max(1, (len(text) - block_size) // (multip - 1))\n",
    "            for i in range(0, min(len(text) - block_size, block_size * (multip - 1) + 1), step):\n",
    "                overlapped_texts.append(text[i: i + block_size])\n",
    "\n",
    "    \n",
    "    encodings = torch.tensor(overlapped_texts, dtype=torch.long)\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc140972-62ce-4137-b5d1-423fc2fda45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_texts = [\n",
    "    '../dataset/clean/generated_curious_pushkin.txt',\n",
    "    '../dataset/clean/generated_pushkin.txt',\n",
    "]\n",
    "\n",
    "fild_texts  = [\n",
    "    '../dataset/clean/original_cleaned.txt',\n",
    "]\n",
    "\n",
    "block_size = 436  # 436+\n",
    "multip = 7\n",
    "train_dataset = load_dataset(splitted_texts, tokenizer, block_size=block_size)\n",
    "len(train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3e22a-80ea-444e-bb2d-627d9712e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(t) for t in train_dataset]\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.title('длины в токенах обрезанных текстов')\n",
    "plt.scatter(np.arange(len(lens)), lens, s=1, c='purple', alpha=0.1)\n",
    "plt.xlabel('тексты')\n",
    "plt.ylabel('длина в токенах')\n",
    "plt.show()\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a89e0-eaaa-49df-b654-e26d2803cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{model_name}_train',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=3_000,\n",
    "    save_total_limit=100,\n",
    "    fp16=True,\n",
    "    logging_dir=f'{model_name}_train/logs', \n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset[:],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18908648-2012-4549-a583-249a63969289",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "path = f'{model_name}_train/{model_name}_{num_epochs}_epochs'\n",
    "\n",
    "trainer.save_model(path)\n",
    "tokenizer.save_pretrained(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21f907-5c02-47a5-9fa9-e847cdc8183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "\n",
    "losses = [i.get('loss') for i in logs]\n",
    "plt.plot(losses)\n",
    "plt.title('losses')\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d17b6-c4d0-44c2-acc0-d9ba759b3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  f'{model_name}_train/{model_name}_{num_epochs}_epochs'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789dec7-c3d1-4384-a15a-4bbdc9115821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, device, tokenizer):\n",
    "    model.to(device)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        min_length=512,\n",
    "        max_length=1024,\n",
    "        num_beams=2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=20, \n",
    "        top_p=0.5, \n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7, \n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b194ef-24a1-4811-8e63-4eec66b1f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ночной волшебный лес то тут то там мелькал радужными неоновыми водопадами и таинственно мерцающими\"\n",
    "generated_texts = generate_text(prompt, device, tokenizer)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e6ca9-09f6-44bf-b45b-3f2df349dca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
