{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4f91e9-3d13-4f88-ab2d-4e91a2ba4834",
   "metadata": {},
   "source": [
    "# использовал gpt, чтобы разобраться в механиках сохранения моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866095e-232a-4948-9b60-f30861c76032",
   "metadata": {},
   "source": [
    "моя сеть будет брать на вход батч из n текстов и учиться на каждом тексте последовательно предугадывать каждый следующий токен изходя из соответсвующего префикса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0212ea5-89bc-460f-a775-89fa1cb9bbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7deed274-28b7-45b5-b4d8-f46808a577b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fba455a-55aa-4351-a101-62f4c4fd7b01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab41c8c-7e87-430f-8d7e-fb8809b64d4a",
   "metadata": {},
   "source": [
    "## загружаю данные, токенайзер, пишу даталодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f9d38c-7f67-4b2a-ae6a-0a3bf0749100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../dataset/generated/tokenizer_and_tokenized_dataset/tokenized_texts.pkl', 'rb') as f:\n",
    "    tokenized_texts = pickle.load(f)\n",
    "\n",
    "tokenizer = Tokenizer.from_file('../dataset/generated/tokenizer_and_tokenized_dataset/tokenizer_2_vocab_1410_made_on_generated_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4a0fa7-bfe4-4f39-8a5f-e3410a8b5aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1410"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2id = tokenizer.get_vocab()\n",
    "id2t = {i : t for t, i in t2id.items()}\n",
    "vocab_size = len(t2id.items())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b51fc18-fb96-47f8-94c8-3465aa49a5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "  def __init__(self, text):\n",
    "    self.data = text\n",
    "    self.unk_id = t2id['<unk>']\n",
    "    self.bos_id = t2id['<bos>']\n",
    "    self.eos_id = t2id['<eos>']\n",
    "    self.pad_id = t2id['<pad>']\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    token_t = [self.bos_id]\n",
    "    token_t += [t2id.get(t, self.unk_id) for t in self.data[idx]]\n",
    "    token_t += [self.eos_id]\n",
    "\n",
    "    return token_t\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1f4b1e-e302-4337-9dce-4d9af3bc87e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trans_func(input_batch, pad_id=t2id['<pad>']):  # [text, text, ...] -> batch\n",
    "    '''\n",
    "    Пример (пусть числа - это номера токенов в словаре):\n",
    "    input_batch = [\n",
    "        [t2id['<bos>'], 14, 53, 23, t2id['<eos>']],\n",
    "        [t2id['<bos>'], 67, 98, t2id['<eos>']],\n",
    "    ]\n",
    "    \n",
    "    фукнция должда вернуть\n",
    "    batch = {\n",
    "        'in_ids': [\n",
    "                    [pad_id ... pad_id, pad_id, pad_id, bos_id],\n",
    "                    [pad_id ... pad_id, pad_id, bos_id, 14],\n",
    "                    [pad_id ... pad_id, bos_id, 14,     53 ],\n",
    "                    [pad_id ... bos_id, 14,     53,     23],\n",
    "\n",
    "                    [pad_id ... pad_id, pad_id, pad_id, bos_id],\n",
    "                    [pad_id ... pad_id, pad_id, bos_id, 67],\n",
    "                    [pad_id ... pad_id, bos_id, 67,     98],\n",
    "\n",
    "                  ]\n",
    "        'target_ids': [14, 53, 23, eos_id, 67, 98, eos_id],\n",
    "    }\n",
    "    '''\n",
    "    max_text_len = max(len(text) for text in input_batch)\n",
    "    seq_cou = sum(len(text) - 1 for text in input_batch)\n",
    "    \n",
    "    in_ids = torch.full((seq_cou, max_text_len), pad_id, dtype=torch.long)\n",
    "    target_ids = torch.empty(seq_cou, dtype=torch.long)\n",
    "\n",
    "    seq_idx = 0\n",
    "    for text in input_batch:\n",
    "        text_tens = torch.tensor(text, dtype=torch.long)\n",
    "        target_ids[seq_idx: seq_idx + text_tens.shape[0] - 1] = text_tens[1:]\n",
    "        \n",
    "        for i in range(1, len(text)):\n",
    "            in_ids[seq_idx, -i:] = text_tens[:i]\n",
    "            # target_ids[seq_idx] = text_tens[i]\n",
    "            seq_idx += 1\n",
    "        \n",
    "\n",
    "    batch = {\n",
    "        'in_ids': in_ids.to(device), \n",
    "        'target_ids': target_ids.to(device),\n",
    "    }\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c65b83-5f86-408a-b58e-7025653b14ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b3a281-6bfe-436b-8795-5f8fcee4c827",
   "metadata": {},
   "source": [
    "# ручками обрезал размер датасета, не забыть об этом "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44831c0-6346-4588-91a6-88f14e1aed71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_texts = random.sample(tokenized_texts, len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cff0f8-e946-4290-b205-9f012b598456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = sorted(tokenized_texts, key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b656d97-7e36-4c27-ac28-2661002acf41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_t, eval_t = train_test_split(tokenized_texts[:300], test_size=1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63144dc5-ccbe-4536-a385-7eedb8123c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_t)\n",
    "eval_dataset = TextDataset(eval_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e13598-d251-4eb3-8ea3-b5fffeda3ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, collate_fn=trans_func, batch_size=2,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=eval_dataset, collate_fn=trans_func, batch_size=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7c578-8552-4579-8994-a21b5b9f7064",
   "metadata": {},
   "source": [
    "## линейная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d719cb2-3b90-47f1-8c0c-1e879696f46b",
   "metadata": {},
   "source": [
    "сделаю линейную модель с переменными гиперпараметрами:\n",
    "\n",
    "$ hidden\\_sizes = [h_1, h_2, ..., h_n], dropouts = [d1, d2, ..., dn], embedding\\_dim $\n",
    "\n",
    "сгенерирую много моделей с разными гиперпараметрами, обучу на x эпохах и выберу лучшую по перплексии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d63401-0e49-450c-b6cf-aa0eb840ab53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearTensorModel(nn.Module):\n",
    "    def __init__(self, hidden_sizes, dropout_values, attantion_count, embedding_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.perplexities = []\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout_values = dropout_values\n",
    "        self.attention_count = attantion_count\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.depth = len(hidden_sizes)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.non_lin = nn.Tanh()\n",
    "        # self.non_lin = nn.ReLU()\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        \n",
    "        last_size = embedding_dim\n",
    "\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            self.linears.append(nn.Linear(last_size, self.hidden_sizes[i]))\n",
    "            self.dropouts.append(nn.Dropout(p=self.dropout_values[i]))\n",
    "            last_size =  self.hidden_sizes[i]\n",
    "\n",
    "        for i in range(self.attention_count):\n",
    "            self.attentions.append(nn.Linear(last_size, 1))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.fc = nn.Linear(last_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input_batch):\n",
    "        out = self.embedding(input_batch)  # (batch_size, seq_len, embedding_dim)\n",
    "    \n",
    "        for i in range(self.depth):\n",
    "            out = self.non_lin(self.dropouts[i](self.linears[i](out)))  # (batch_size, seq_len, self.hidden_sizes[i])\n",
    "\n",
    "        weighted_outs = []\n",
    "        for i in range(self.attention_count):\n",
    "            weights = self.attentions[i](out)\n",
    "            weights = self.softmax(weights)\n",
    "            weighted_outs.append(torch.sum(weights*out, dim=1))\n",
    "        \n",
    "        out = sum(weighted_outs) / self.attention_count  # (batch_size, self.hidden_sizes[-1])\n",
    "\n",
    "        out = self.fc(out)  # (batch_size, vocab_size)\n",
    "        return out\n",
    "\n",
    "    def get_sizes(self):\n",
    "        return {\n",
    "            'hidden_sizes': self.hidden_sizes,\n",
    "            'dropout_values': self.dropout_values,\n",
    "            'embedding_dim': self.embedding_dim, \n",
    "            'attention_count': self.attention_count,\n",
    "            'vocab_size': self.vocab_size, \n",
    "            'deepht': self.depth, \n",
    "        }\n",
    "\n",
    "    def get_losses(self):\n",
    "        return self.losses\n",
    "\n",
    "    def get_perplexities(self):\n",
    "        return self.perplexities\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'hs={self.hidden_sizes}, ed={self.embedding_dim}, ac={self.attention_count}'\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552b91e-b6d6-4402-8a20-bc6c612e7879",
   "metadata": {},
   "source": [
    "## evaluate, train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c6cad-cc3c-45c6-939c-f31bec335b22",
   "metadata": {},
   "source": [
    "тк я планирю делать сразу много моделей, я сделаю эти методы так, чтобы они работали не на 1 модели, а на списке:\n",
    "\n",
    "models = [(model1, optimizer1), (model2, optimizer2), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27830206-dec2-4ff7-8e60-7b2b41c69dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, optimizer, crit, eval_dataloader):\n",
    "    model.eval()\n",
    "    perplexity = []\n",
    "    amount = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            logs = model(batch['in_ids'])\n",
    "            len_batch = len(batch['in_ids'])\n",
    "            for i in range(len_batch):\n",
    "                loss = crit(\n",
    "                    logs[i], \n",
    "                    batch['target_ids'][i]\n",
    "                )\n",
    "                perplexity.append(torch.exp(loss).item())\n",
    "    rate = sum(perplexity) / len(perplexity)\n",
    "    return rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f213e9ea-3ea2-4a67-a900-4385b1a737f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crit_func = nn.CrossEntropyLoss(ignore_index=t2id['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f1a4c-39f0-46c3-9949-5c92e9102911",
   "metadata": {},
   "source": [
    "### для того, чтобы выбрать лучшую архитектуру, создаю и обучаю на не большой подвыборке кластер с рандомными гиперпараметрами (hidden_dim, hidden_sizes, dropout_values, embedding_dim) на не большом количестве эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f54ba9-c76e-4dd9-b232-c1d0d8f90bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filepath):\n",
    "\n",
    "    model_state = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'losses': model.get_losses(),\n",
    "        'perplexities': model.get_perplexities(),\n",
    "        'hidden_sizes': model.hidden_sizes,\n",
    "        'dropout_values': model.dropout_values,\n",
    "        'attention_count': model.attention_count,\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'vocab_size': model.vocab_size\n",
    "    }\n",
    "\n",
    "    torch.save(model_state, filepath)\n",
    "\n",
    "def load_model(filepath, model_class):\n",
    "\n",
    "    model_state = torch.load(filepath, weights_only=False)\n",
    "    \n",
    "    model = LinearTensorModel(\n",
    "        hidden_sizes=model_state['hidden_sizes'],\n",
    "        dropout_values=model_state['dropout_values'],\n",
    "        attantion_count=model_state['attention_count'],\n",
    "        embedding_dim=model_state['embedding_dim'],\n",
    "        vocab_size=model_state['vocab_size']\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(model_state['model_state_dict'])\n",
    "    \n",
    "    model.losses = model_state['losses']\n",
    "    model.perplexities = model_state['perplexities']\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42128a23-8e22-432e-b81e-f74b99db75f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "already_done = 0\n",
    "claster_size = 20 - already_done \n",
    "\n",
    "# hidden_dim_vals = (1, 4)\n",
    "# hidden_dim_sizes_vals = (4, 64)\n",
    "# dropout_valuse_vals = (0, 0.2)\n",
    "# embedding_dim_vals = (30, 300)\n",
    "# attantion_count_vals = (1, 5)\n",
    "\n",
    "\n",
    "hidden_dim_vals = (1, 4)\n",
    "hidden_dim_sizes_vals = (4, 32)\n",
    "dropout_valuse_vals = (0, 0.2)\n",
    "embedding_dim_vals = (30, 100)\n",
    "attantion_count_vals = (1, 5)\n",
    "\n",
    "claster = []\n",
    "\n",
    "for i in range(claster_size):\n",
    "    hidden_dim = random.randint(hidden_dim_vals[0], hidden_dim_vals[1])\n",
    "    hidden_sizes = random.choices(range(hidden_dim_sizes_vals[0], hidden_dim_sizes_vals[1] + 1), k=hidden_dim)\n",
    "    dropout_values = np.random.uniform(dropout_valuse_vals[0], dropout_valuse_vals[1], hidden_dim)\n",
    "    attantion_count = random.randint(attantion_count_vals[0], attantion_count_vals[1])\n",
    "    embedding_dim = random.randint(embedding_dim_vals[0], embedding_dim_vals[1])\n",
    "    \n",
    "    m = LinearTensorModel(hidden_sizes=hidden_sizes, dropout_values=dropout_values, attantion_count=attantion_count, embedding_dim=embedding_dim, vocab_size=vocab_size)\n",
    "    o = torch.optim.Adam(m.parameters())\n",
    "    claster.append((m.to(device), o))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c624caa-13be-4446-aaac-657fd33632d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a3b135ad8042b89160f0c3daae5b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model: 1/1, epoch: 1/5, params: hs=[8, 30, 27], ed=94, ac=2:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d3b48ff26d47dcb27d9fb24df154ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model: 1/1, epoch: 2/5, params: hs=[8, 30, 27], ed=94, ac=2:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4509dc6c4bb94ee29969f3fc9ac5ed86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model: 1/1, epoch: 3/5, params: hs=[8, 30, 27], ed=94, ac=2:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9c905a40ac4059b94df9de00581fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model: 1/1, epoch: 4/5, params: hs=[8, 30, 27], ed=94, ac=2:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd407364578450faf46636fa29fc6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model: 1/1, epoch: 5/5, params: hs=[8, 30, 27], ed=94, ac=2:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_model_claster\n",
    "num_ep = 20\n",
    "models_count = len(claster)\n",
    "\n",
    "for model_idx in range(models_count):\n",
    "    model = claster[model_idx][0]\n",
    "    optimizer = claster[model_idx][1]\n",
    "    losses = []\n",
    "    perplexities = []\n",
    "    try:\n",
    "        for ep in range(num_ep):\n",
    "            model.train()\n",
    "            ep_losses=[]\n",
    "            for batch in tqdm(train_dataloader, desc=f'model: {model_idx + 1}/{models_count}, epoch: {ep + 1}/{num_ep}, params: {claster[model_idx][0]}: '):\n",
    "                optimizer.zero_grad()\n",
    "                logs = model(batch['in_ids'])\n",
    "                loss = crit_func(\n",
    "                    logs, batch['target_ids']\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                ep_losses.append(loss.item())\n",
    "            losses.append(sum(ep_losses) / len(ep_losses))\n",
    "            perplexities.append(evaluate(model, optimizer, crit_func, eval_dataloader))\n",
    "            \n",
    "            model.losses = losses\n",
    "            model.perplexities = perplexities\n",
    "    \n",
    "    \n",
    "        # saving models:\n",
    "        save_model(claster[model_idx][0].to('cpu'), f'simple_linear_models/linear_rand_model_{model_idx + already_done + 1}_epochs_{num_ep}.pth')\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt')\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900737d-738d-4206-8704-08c341f6f775",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "загружаю все модели из сохранений\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33539945-c944-4d10-9cf7-07697087df78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearTensorModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple_linear_models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m [load_model(folder_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m path, \u001b[43mLinearTensorModel\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearTensorModel' is not defined"
     ]
    }
   ],
   "source": [
    "folder_path = 'simple_linear_models'\n",
    "models = [load_model(folder_path + '/' + path, LinearTensorModel) for path in os.listdir(folder_path)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c5a3f0-4146-494c-91ff-5bb6a90decd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw_end_losses_and_perplexities_for_claster(models):\n",
    "    end_losses = [m.get_losses()[-1] for m in models]\n",
    "    end_perplexities = [m.get_perplexities()[-1] for m in models]\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.scatter(np.arange(len(end_losses)), end_losses, s=100)\n",
    "    plt.title('losses')\n",
    "    plt.xlabel('models')\n",
    "    plt.ylabel('loss_val')\n",
    "    \n",
    "    plt.xticks(np.arange(len(end_losses)))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.scatter(np.arange(len(end_perplexities)), end_perplexities, s=100)\n",
    "    plt.title('perplexities')\n",
    "    plt.xlabel('models')\n",
    "    plt.ylabel('perplexities_val')\n",
    "    \n",
    "    plt.xticks(np.arange(len(end_perplexities)))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_all_losses_and_perplexities_for_claster(models):\n",
    "    epoches_num = len(models[0].get_losses())\n",
    "    #  losses\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.title('losses change')\n",
    "    plt.xlabel('epchs')\n",
    "    plt.ylabel('loss_val')\n",
    "    for i in range(len(models)):\n",
    "        plt.plot(models[i].get_losses(), label=f'model[{i}]')\n",
    "        \n",
    "        \n",
    "    plt.xticks(np.arange(epoches_num))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #  perpleixties\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.title('perpleixties change')\n",
    "    plt.xlabel('epchs')\n",
    "    plt.ylabel('loss_val')\n",
    "    for i in range(len(models)):\n",
    "        plt.plot(models[i].get_perplexities(), label=f'model[{i}]')\n",
    "        \n",
    "        \n",
    "    plt.xticks(np.arange(epoches_num))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69c0e35-74a6-4da7-b7c4-f7dda8dfe455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_end_losses_and_perplexities_for_claster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdraw_end_losses_and_perplexities_for_claster\u001b[49m(models)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_end_losses_and_perplexities_for_claster' is not defined"
     ]
    }
   ],
   "source": [
    "draw_end_losses_and_perplexities_for_claster(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abda2e9e-1004-4e88-816e-416ad199a8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_all_losses_and_perplexities_for_claster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdraw_all_losses_and_perplexities_for_claster\u001b[49m(models)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_all_losses_and_perplexities_for_claster' is not defined"
     ]
    }
   ],
   "source": [
    "draw_all_losses_and_perplexities_for_claster(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9c854-0174-4590-9c28-ce2da57780bf",
   "metadata": {},
   "source": [
    "результат генерации различных моделей сомнительный, все работают плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea4d6f-45e3-4264-a654-6aeb3a05769d",
   "metadata": {},
   "source": [
    "## text generation func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b776e99-4ca8-404f-9345-93a1153162b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, start_s='жили у бабуси ', max_s_len=100, tokenizer=tokenizer, t2id=t2id, id2t=id2t):\n",
    "    device = 'cuda'\n",
    "    model.to(device)\n",
    "    input_ids = [t2id['<bos>']] + [t2id.get(t, t2id['<unk>']) for t in tokenizer.encode(start_s).tokens]\n",
    "    input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_s_len):\n",
    "            next_t_vec = model(input_ids)\n",
    "            next_t = next_t_vec.squeeze().argmax()\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_t.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            # print(next_t.item(), input_ids)\n",
    "            if next_t == t2id['<eos>']:\n",
    "                break\n",
    "    text = ''.join([id2t[ind.item()] for ind in input_ids[0]])\n",
    "    return text\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba92a91f-4577-4c30-8fb0-ccade15f3889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>и днем и ночьюжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилжилщижилжилжилщижилжилщижилжилжилщижилжилжилщижилжилщижилжилжилщижилжилжилщижилжилжилщижилжилщижилжилжилщижилжилжилщижилжилщижилжилжилщижилжилжилщижилжилщижилжилжилщижилжилжилщижилжилщижилжил'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(models[0],'и днем и ночью', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea3dc1-868a-4623-9956-e926d60e89a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4df71-cf40-4ec9-a5e6-c1d40af211f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9fd1b-8da6-425e-8545-2f8ecb53c4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225be11-708c-452e-8cde-8e2b032c9d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a9289-1822-46d1-ba48-b075c9585df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06e713-68fe-46a9-bb34-1068d7c2b80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd9895-ddbe-4235-a8e9-a31d7332e3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
